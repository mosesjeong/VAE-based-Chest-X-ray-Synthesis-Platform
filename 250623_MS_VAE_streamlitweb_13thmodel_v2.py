import streamlit as st
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import os

# ----------------------------------------------------------------------
# 1. í˜ì´ì§€ ì„¤ì • ë° ê¸°ë³¸ êµ¬ì„±
# ----------------------------------------------------------------------

# Streamlit í˜ì´ì§€ì˜ ì œëª©, ì•„ì´ì½˜, ë ˆì´ì•„ì›ƒ ë“± ê¸°ë³¸ ì„¤ì •ì„ ì§€ì •í•©ë‹ˆë‹¤.
st.set_page_config(
    page_title="VAE & Diffusion Image Generator",
    page_icon="ğŸ¨",
    layout="wide",
)

# ----------------------------------------------------------------------
# 2. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜
# í•™ìŠµ ì‹œ ì‚¬ìš©ëœ ëª¨ë¸ê³¼ ë™ì¼í•œ êµ¬ì¡°ë¡œ ì •ì˜í•´ì•¼ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ----------------------------------------------------------------------

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° (í•™ìŠµ ë‹¹ì‹œì™€ ë™ì¼í•´ì•¼ í•¨) ---
IMG_SIZE = 64
LATENT_DIM = 512
ENCODER_CHANNELS = [64, 128, 256, 512]
DECODER_CHANNELS = [256, 128, 64, 32]
DENOISER_CHANNELS = [512, 256, 128, 64]


def build_encoder(latent_dim, channels):
    """ì´ë¯¸ì§€ë¥¼ ì ì¬ ë²¡í„°ë¡œ ì••ì¶•í•˜ëŠ” ì¸ì½”ë” ëª¨ë¸ì„ ë¹Œë“œí•©ë‹ˆë‹¤."""
    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    x = inputs
    for ch in channels:
        x = layers.Conv2D(ch, kernel_size=4, strides=2, padding='same', use_bias=False)(x)
        x = layers.BatchNormalization()(x)
        x = layers.LeakyReLU(alpha=0.2)(x)
    x = layers.Flatten()(x)
    mean = layers.Dense(latent_dim, name="mean")(x)
    log_var = layers.Dense(latent_dim, name="log_var")(x)
    return keras.Model(inputs, [mean, log_var], name="encoder")


def build_decoder(latent_dim, channels):
    """ì ì¬ ë²¡í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë³µì›í•˜ëŠ” ë””ì½”ë” ëª¨ë¸ì„ ë¹Œë“œí•©ë‹ˆë‹¤."""
    inputs = layers.Input(shape=(latent_dim,))
    x = layers.Dense(4 * 4 * channels[0])(inputs)
    x = layers.Reshape((4, 4, channels[0]))(x)
    for ch in channels:
        x = layers.Conv2DTranspose(ch, kernel_size=4, strides=2, padding='same', use_bias=False)(x)
        x = layers.BatchNormalization()(x)
        x = layers.LeakyReLU(alpha=0.2)(x)
    outputs = layers.Conv2D(3, kernel_size=5, padding='same', activation='tanh')(x)
    return keras.Model(inputs, outputs, name="decoder")


def build_denoiser(latent_dim, channels):
    """ë…¸ì´ì¦ˆê°€ ë‚€ ì ì¬ ë²¡í„°ì—ì„œ ë…¸ì´ì¦ˆë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë””ë…¸ì´ì € ëª¨ë¸ì„ ë¹Œë“œí•©ë‹ˆë‹¤."""
    latent_input = layers.Input(shape=(latent_dim,))
    time_input = layers.Input(shape=(1,))
    time_embedding = layers.Dense(latent_dim, activation='swish')(time_input)
    x = layers.Add()([latent_input, time_embedding])
    x = layers.Dense(latent_dim, activation='swish')(x)
    for ch in channels:
        x_res = x
        x = layers.Dense(ch, activation='swish')(x)
        x = layers.BatchNormalization()(x)
        if x_res.shape[-1] != ch:
            x_res = layers.Dense(ch)(x_res)
        x = layers.Add()([x, x_res])
    output = layers.Dense(latent_dim)(x)
    return keras.Model([latent_input, time_input], output, name="denoiser")


class VAE(keras.Model):
    """ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ê²°í•©í•œ VAE ëª¨ë¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""

    def __init__(self, encoder, decoder, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder


class DiffusionModel(keras.Model):
    """ë””ë…¸ì´ì €ë¥¼ í¬í•¨í•˜ëŠ” Diffusion ëª¨ë¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""

    def __init__(self, denoiser, **kwargs):
        super().__init__(**kwargs)
        self.denoiser = denoiser


# ----------------------------------------------------------------------
# 3. ëª¨ë¸ ë¡œë”© ë° í•µì‹¬ ê¸°ëŠ¥ í•¨ìˆ˜
# ----------------------------------------------------------------------

@st.cache_resource
def load_models():
    """ë¯¸ë¦¬ í•™ìŠµëœ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    @st.cache_resourceë¥¼ ì‚¬ìš©í•˜ì—¬ ì•± ì‹¤í–‰ ì¤‘ í•œ ë²ˆë§Œ ë¡œë“œë˜ë„ë¡ í•©ë‹ˆë‹¤."""
    # ëª¨ë¸ êµ¬ì¡° ë¹Œë“œ
    encoder = build_encoder(LATENT_DIM, ENCODER_CHANNELS)
    decoder = build_decoder(LATENT_DIM, DECODER_CHANNELS)
    denoiser = build_denoiser(LATENT_DIM, DENOISER_CHANNELS)

    # VAE ë° Diffusion ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    vae_model = VAE(encoder, decoder)
    diffusion_model = DiffusionModel(denoiser)

    # ì €ì¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    # ì‚¬ìš©ìëŠ” ì—¬ê¸°ì— ì‹¤ì œ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    vae_weights_path = 'vae_epoch_30.h5'
    diffusion_weights_path = 'diffusion_epoch_30.h5'

    if os.path.exists(vae_weights_path) and os.path.exists(diffusion_weights_path):
        # build a dummy input to initialize model weights
        dummy_img = tf.random.normal([1, IMG_SIZE, IMG_SIZE, 3])
        vae_model(dummy_img)
        dummy_latent = tf.random.normal([1, LATENT_DIM])
        dummy_time = tf.constant([0.5], dtype=tf.float32)
        diffusion_model([dummy_latent, dummy_time])

        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        vae_model.load_weights(vae_weights_path)
        diffusion_model.load_weights(diffusion_weights_path)
        print("ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì™„ë£Œ.")
    else:
        # ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì—†ì„ ê²½ìš° ê²½ê³  ë©”ì‹œì§€ í‘œì‹œ
        st.error(f"ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{vae_weights_path}', '{diffusion_weights_path}'")
        return None, None

    return vae_model, diffusion_model


def get_random_latent_vector(num_images):
    """ì§€ì •ëœ ê°œìˆ˜ë§Œí¼ ë¬´ì‘ìœ„ ì ì¬ ë²¡í„°(ì‹œë“œ)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return tf.random.normal(shape=(num_images, LATENT_DIM))


def generate_images(num_images, steps, latent_vector_seed):
    """ì ì¬ ë²¡í„° ì‹œë“œë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    vae, diffusion = load_models()
    if vae is None:
        return []

    # ì…ë ¥ëœ ì‹œë“œë¥¼ í˜„ì¬ ì ì¬ ë²¡í„°ë¡œ ì„¤ì •
    current_latents = latent_vector_seed

    # Diffusionì˜ ì—­ë°©í–¥ í”„ë¡œì„¸ìŠ¤ (ë…¸ì´ì¦ˆ ì œê±° ê³¼ì •)
    # ì´ `steps`ì— ê±¸ì³ ì ì§„ì ìœ¼ë¡œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•©ë‹ˆë‹¤.
    for t_step in np.linspace(1.0, 0.0, steps):
        t = tf.constant([t_step] * num_images, dtype=tf.float32)
        predicted_noise = diffusion([current_latents, t])
        # ë…¸ì´ì¦ˆ ì œê±° ê°•ë„ë¥¼ ìŠ¤í… ìˆ˜ì— ë”°ë¼ ì¡°ì ˆ
        alpha = 1.0 / steps
        current_latents -= alpha * predicted_noise

    # ìµœì¢…ì ìœ¼ë¡œ ë…¸ì´ì¦ˆê°€ ì œê±°ëœ ì ì¬ ë²¡í„°ë¥¼ ë””ì½”ë”ì— í†µê³¼ì‹œì¼œ ì´ë¯¸ì§€ ìƒì„±
    generated_images = vae.decoder(current_latents)
    return generated_images


# ----------------------------------------------------------------------
# 4. Streamlit UI êµ¬ì„±
# ----------------------------------------------------------------------

st.title("ğŸ¨ VAE-Diffusion ì´ë¯¸ì§€ ìƒì„±ê¸°")
st.markdown("""
ì´ ì›¹ ì•±ì€ **VAE(Variational Autoencoder)**ì™€ **Diffusion Model**ì„ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ì‚¬ì´ë“œë°”ì—ì„œ ìƒì„±í•  ì´ë¯¸ì§€ì˜ ìˆ˜, ë…¸ì´ì¦ˆ ì œê±° ë‹¨ê³„(Steps), ê·¸ë¦¬ê³  ìƒì„± ì‹œë“œ(Seed)ë¥¼ ì¡°ì ˆí•˜ì—¬ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”.
""")

# --- ì‚¬ì´ë“œë°” ì»¨íŠ¸ë¡¤ ---
with st.sidebar:
    st.header("âš™ï¸ ì»¨íŠ¸ë¡¤ íŒ¨ë„")

    # ìƒì„±í•  ì´ë¯¸ì§€ ê°œìˆ˜ ì¡°ì ˆ ìŠ¬ë¼ì´ë”
    num_images_to_gen = st.slider("ìƒì„±í•  ì´ë¯¸ì§€ ê°œìˆ˜", 1, 16, 4)

    # ë…¸ì´ì¦ˆ ì œê±° ë‹¨ê³„ ì¡°ì ˆ ìŠ¬ë¼ì´ë”
    steps = st.slider("ìƒì„± ìŠ¤í… (Steps)", 10, 100, 50)

    st.markdown("---")
    st.subheader("ğŸŒ± ìƒì„± ì‹œë“œ (Seed)")

    # session_stateë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œë“œ ê°’ì„ ì—¬ëŸ¬ ì¸í„°ë™ì…˜ì— ê±¸ì³ ìœ ì§€
    if 'latent_seed' not in st.session_state:
        st.session_state.latent_seed = get_random_latent_vector(num_images_to_gen)

    # ìƒˆë¡œìš´ ë¬´ì‘ìœ„ ì‹œë“œ ìƒì„± ë²„íŠ¼
    if st.button("ìƒˆë¡œìš´ ëœë¤ ì‹œë“œ ìƒì„±"):
        st.session_state.latent_seed = get_random_latent_vector(num_images_to_gen)
        st.success("ìƒˆë¡œìš´ ëœë¤ ì‹œë“œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")

    # ì‚¬ìš©ìê°€ ì‹œë“œ ê°’ì„ í…ìŠ¤íŠ¸ë¡œ ì§ì ‘ ì…ë ¥
    seed_input = st.text_area("ë˜ëŠ” ì‹œë“œ ê°’ì„ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš” (ì‰¼í‘œë¡œ êµ¬ë¶„)",
                              value=", ".join(map(str, st.session_state.latent_seed[0, :5].numpy())) + ", ...")

# --- ë©”ì¸ í™”ë©´ ---
st.markdown("---")

# ì´ë¯¸ì§€ ìƒì„±ì„ íŠ¸ë¦¬ê±°í•˜ëŠ” ë©”ì¸ ë²„íŠ¼
if st.button("ğŸ–¼ï¸ ì´ë¯¸ì§€ ìƒì„±í•˜ê¸° (GENERATE)"):
    # ì…ë ¥ëœ ì‹œë“œ ê°’ íŒŒì‹± (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¬´ì‘ìœ„ ì‹œë“œ ì‚¬ìš©)
    try:
        # ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸ ì‹œë“œë¥¼ ë‹¤ì‹œ í…ì„œë¡œ ë³€í™˜
        seed_values = [float(x.strip()) for x in seed_input.split(',') if x.strip()]
        if len(seed_values) == LATENT_DIM:
            current_seed = tf.constant([seed_values] * num_images_to_gen, dtype=tf.float32)
        else:
            # ì‚¬ìš©ìê°€ ì¼ë¶€ë§Œ ì…ë ¥í–ˆê±°ë‚˜ ì˜ëª» ì…ë ¥í•œ ê²½ìš°, session_stateì˜ ì‹œë“œë¥¼ ì‚¬ìš©
            current_seed = st.session_state.latent_seed
            st.warning(f"ì…ë ¥ëœ ì‹œë“œ ê°’ì´ ìœ íš¨í•˜ì§€ ì•Šì•„ ê¸°ì¡´ ì‹œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. {LATENT_DIM}ê°œì˜ ìˆ«ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    except:
        current_seed = st.session_state.latent_seed
        st.error("ì‹œë“œ ê°’ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ì‹œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    # ë¡œë”© ìŠ¤í”¼ë„ˆì™€ í•¨ê»˜ ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ
    with st.spinner('ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.'):
        generated_images = generate_images(num_images_to_gen, steps, current_seed)

    # ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ í™”ë©´ì— í‘œì‹œ
    if len(generated_images) > 0:
        st.success("ì´ë¯¸ì§€ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        # ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ì—´ì— ê±¸ì³ ë³´ê¸° ì¢‹ê²Œ í‘œì‹œ
        cols = st.columns(4)
        for i, img_tensor in enumerate(generated_images):
            # í…ì„œ ê°’ì„ [0, 1] ë²”ìœ„ì˜ numpy ë°°ì—´ë¡œ ë³€í™˜
            img_array = (img_tensor.numpy() + 1) / 2.0
            cols[i % 4].image(img_array, caption=f"Image {i + 1}", use_column_width=True)
    else:
        st.error("ì´ë¯¸ì§€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”.")