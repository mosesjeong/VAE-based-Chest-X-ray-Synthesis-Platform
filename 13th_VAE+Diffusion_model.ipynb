{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38526e18-97f7-4e50-9160-7cc0c213d0cf",
   "metadata": {},
   "source": [
    "# VAE와 Diffusion을 결합한 이미지 생성 모델\n",
    "\n",
    "**프로젝트 목표:**\n",
    "- VAE(Variational Autoencoder)를 사용하여 고차원의 이미지를 저차원의 잠재 공간(Latent Space)으로 압축하고, Diffusion 모델을 사용하여 이 잠재 공간에서 노이즈를 점진적으로 제거하며 새로운 이미지를 생성합니다.\n",
    "- L1 손실과 지각 손실(Perceptual Loss)을 함께 사용하여 생성된 이미지의 품질을 높입니다.\n",
    "\n",
    "**수행 과정:**\n",
    "1.  **설정:** 이미지 크기, 배치 사이즈, 잠재 공간 차원 등 주요 하이퍼파라미터를 설정합니다.\n",
    "2.  **데이터 준비:** 이미지 데이터셋을 불러오고 모델에 입력할 수 있도록 전처리합니다.\n",
    "3.  **모델 정의:** Encoder, Decoder, Denoiser, VAE, Diffusion 모델의 각 구성 요소를 정의합니다.\n",
    "4.  **손실 함수 및 최적화:** 모델 학습에 사용될 손실 함수와 Optimizer를 정의합니다.\n",
    "5.  **학습 루프:** 정의된 모델과 손실 함수를 사용하여 Epoch별로 학습을 진행하고, 중간 결과를 시각화하여 저장합니다.\n",
    "6.  **최종 이미지 생성:** 학습이 완료된 모델을 사용하여 최종 결과 이미지를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bbc59b-494d-4199-b421-7a6e4d218f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 경고 메시지 무시 (선택 사항)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a76b89-ee6f-47bd-9334-2d93f55ddc68",
   "metadata": {},
   "source": [
    "## 1. 하이퍼파라미터 및 전역 설정\n",
    "모델 학습과 데이터 처리에 필요한 주요 변수들을 한곳에서 관리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28141f-b136-4587-9a0e-23ffa03b47c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 데이터 관련 설정 ---\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "DATASET_PATH = \"./img_align_celeba/img_align_celeba/*.jpg\"\n",
    "\n",
    "# --- 모델 아키텍처 설정 ---\n",
    "LATENT_DIM = 512 # VAE의 잠재 공간 차원\n",
    "ENCODER_CHANNELS = [64, 128, 256, 512]\n",
    "DECODER_CHANNELS = [256, 128, 64, 32]\n",
    "DENOISER_CHANNELS = [512, 256, 128, 64] # Denoiser의 채널 구성\n",
    "\n",
    "# --- 학습 관련 설정 ---\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 2e-4\n",
    "L1_LOSS_WEIGHT = 1.0       # L1 손실 가중치\n",
    "PERCEPTUAL_LOSS_WEIGHT = 0.1 # 지각 손실 가중치\n",
    "\n",
    "# --- 결과 저장 설정 ---\n",
    "SAVE_DIR = \"generated_images_vae_diffusion_v13\"\n",
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85f655-4c2d-45af-8fb3-d603416124d1",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 준비\n",
    "지정된 경로에서 이미지 파일들을 불러와 `tf.data.Dataset`으로 만듭니다. 이미지를 리사이즈하고, 값을 [-1, 1] 범위로 정규화하며, 배치 단위로 묶어 학습에 사용할 수 있도록 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe15d2b-437f-4090-a2bd-3dfee1b5f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로에서 파일 목록 가져오기\n",
    "image_paths = glob.glob(DATASET_PATH)\n",
    "print(f\"총 {len(image_paths)}개의 이미지 발견.\")\n",
    "\n",
    "# tf.data.Dataset 파이프라인 구성\n",
    "def preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    # 픽셀 값을 [0, 255]에서 [-1, 1] 범위로 정규화\n",
    "    image = (tf.cast(image, tf.float32) - 127.5) / 127.5\n",
    "    return image\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "    .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size=1000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "print(\"TensorFlow 데이터셋 준비 완료.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e90597-ab78-4847-b215-5a0ed2f62298",
   "metadata": {},
   "source": [
    "## 3. 모델 아키텍처 정의\n",
    "VAE와 Diffusion 모델의 핵심 구성 요소인 Encoder, Decoder, Denoiser를 각각 정의합니다.\n",
    "\n",
    "- **Encoder**: 이미지를 입력받아 잠재 벡터(평균, 로그 분산)로 압축합니다.\n",
    "- **Decoder**: 잠재 벡터를 입력받아 다시 이미지로 복원합니다.\n",
    "- **Denoiser**: 노이즈가 섞인 잠재 벡터와 노이즈 레벨(시간)을 입력받아 노이즈를 예측합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ea221-9c50-4231-894c-30d688ffa60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 모델 정의\n",
    "def build_encoder(latent_dim, channels):\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = inputs\n",
    "    for ch in channels:\n",
    "        x = layers.Conv2D(ch, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    mean = layers.Dense(latent_dim, name=\"mean\")(x)\n",
    "    log_var = layers.Dense(latent_dim, name=\"log_var\")(x)\n",
    "    return keras.Model(inputs, [mean, log_var], name=\"encoder\")\n",
    "\n",
    "# 디코더 모델 정의\n",
    "def build_decoder(latent_dim, channels):\n",
    "    inputs = layers.Input(shape=(latent_dim,))\n",
    "    # Conv2DTranspose의 입력에 맞게 차원 재구성\n",
    "    x = layers.Dense(4 * 4 * channels[0])(inputs)\n",
    "    x = layers.Reshape((4, 4, channels[0]))(x)\n",
    "    for i, ch in enumerate(channels):\n",
    "        x = layers.Conv2DTranspose(ch, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    # 마지막 레이어에서 채널 수를 3(RGB)으로 맞추고, tanh 활성화 함수로 픽셀 값 범위를 [-1, 1]로 조정\n",
    "    outputs = layers.Conv2D(3, kernel_size=5, padding='same', activation='tanh')(x)\n",
    "    return keras.Model(inputs, outputs, name=\"decoder\")\n",
    "\n",
    "# 디노이저(U-Net 기반) 모델 정의\n",
    "def build_denoiser(latent_dim, channels):\n",
    "    latent_input = layers.Input(shape=(latent_dim,))\n",
    "    time_input = layers.Input(shape=(1,))\n",
    "\n",
    "    # 시간 정보를 임베딩\n",
    "    time_embedding = layers.Dense(latent_dim, activation='swish')(time_input)\n",
    "    \n",
    "    # 입력 잠재 벡터와 시간 임베딩 결합\n",
    "    x = layers.Add()([latent_input, time_embedding])\n",
    "    x = layers.Dense(latent_dim, activation='swish')(x)\n",
    "    \n",
    "    for ch in channels:\n",
    "        x_res = x\n",
    "        x = layers.Dense(ch, activation='swish')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        # Residual Connection을 위해 차원을 맞춤\n",
    "        if x_res.shape[-1] != ch:\n",
    "            x_res = layers.Dense(ch)(x_res)\n",
    "        x = layers.Add()([x, x_res])\n",
    "\n",
    "    output = layers.Dense(latent_dim)(x)\n",
    "    return keras.Model([latent_input, time_input], output, name=\"denoiser\")\n",
    "\n",
    "# 모델 빌드\n",
    "encoder = build_encoder(LATENT_DIM, ENCODER_CHANNELS)\n",
    "decoder = build_decoder(LATENT_DIM, DECODER_CHANNELS)\n",
    "denoiser = build_denoiser(LATENT_DIM, DENOISER_CHANNELS)\n",
    "\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "denoiser.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d1c5a4-7316-4aec-aef1-de68bfca0b2f",
   "metadata": {},
   "source": [
    "## 4. VAE 및 Diffusion 모델 클래스 정의\n",
    "앞서 정의한 아키텍처들을 결합하여 VAE와 Diffusion 모델 전체를 클래스로 정의합니다.\n",
    "\n",
    "- **VAE**: Encoder와 Decoder를 포함하며, Reparameterization Trick을 구현합니다.\n",
    "- **DiffusionModel**: Denoiser를 사용하여 노이즈를 예측하고, 학습 시 노이즈를 추가하는 로직(Forward Process)을 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f08ea6-1b8c-4d37-ab71-67be89efb837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    # Reparameterization Trick: z = mean + exp(0.5 * log_var) * epsilon\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        epsilon = tf.random.normal(shape=tf.shape(mean))\n",
    "        return mean + tf.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = self.encoder(inputs)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mean, log_var\n",
    "\n",
    "class DiffusionModel(keras.Model):\n",
    "    def __init__(self, denoiser, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.denoiser = denoiser\n",
    "\n",
    "    def call(self, inputs):\n",
    "        noisy_latents, time = inputs\n",
    "        # Denoiser는 추가된 노이즈를 예측\n",
    "        predicted_noise = self.denoiser([noisy_latents, time])\n",
    "        return predicted_noise\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "vae = VAE(encoder, decoder)\n",
    "diffusion_model = DiffusionModel(denoiser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd08fa80-97f8-42c7-af3d-b540b16cbad1",
   "metadata": {},
   "source": [
    "## 5. 손실 함수 및 Optimizer 정의\n",
    "모델 학습에 필요한 손실 함수들과 Optimizer를 설정합니다.\n",
    "\n",
    "- **VAE Loss**: 재구성 손실(Reconstruction Loss)과 KL 발산(KL Divergence)으로 구성됩니다.\n",
    "- **Perceptual Loss**: 이미지의 고차원 특징(Feature) 공간에서의 유사도를 측정하는 손실입니다. 시각적으로 더 자연스러운 이미지를 생성하는 데 도움을 줍니다. 이를 위해 미리 학습된 VGG19 모델을 사용합니다.\n",
    "- **Optimizer**: Adam Optimizer를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e7504f-7ac3-41d6-a884-74be7e2ee32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG19 모델을 이용한 지각 손실(Perceptual Loss) 계산\n",
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "vgg.trainable = False\n",
    "perceptual_loss_model = tf.keras.Model(vgg.input, vgg.get_layer(\"block3_conv4\").output)\n",
    "\n",
    "def get_perceptual_loss(y_true, y_pred):\n",
    "    y_true = (y_true + 1) * 127.5 # [-1, 1] -> [0, 255]\n",
    "    y_pred = (y_pred + 1) * 127.5 # [-1, 1] -> [0, 255]\n",
    "    y_true = tf.keras.applications.vgg19.preprocess_input(y_true)\n",
    "    y_pred = tf.keras.applications.vgg19.preprocess_input(y_pred)\n",
    "    true_features = perceptual_loss_model(y_true)\n",
    "    pred_features = perceptual_loss_model(y_pred)\n",
    "    return tf.reduce_mean(tf.square(true_features - pred_features))\n",
    "\n",
    "# VAE 손실 함수 (재구성 손실 + KL 발산)\n",
    "def vae_loss_fn(original_images, reconstructed_images, mean, log_var):\n",
    "    l1_loss = tf.reduce_mean(tf.abs(original_images - reconstructed_images))\n",
    "    perceptual_loss = get_perceptual_loss(original_images, reconstructed_images)\n",
    "    reconstruction_loss = L1_LOSS_WEIGHT * l1_loss + PERCEPTUAL_LOSS_WEIGHT * perceptual_loss\n",
    "    \n",
    "    kl_loss = -0.5 * tf.reduce_sum(1 + log_var - tf.square(mean) - tf.exp(log_var), axis=-1)\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "    \n",
    "    return reconstruction_loss + kl_loss\n",
    "\n",
    "# Diffusion 모델 손실 함수 (Mean Squared Error)\n",
    "mse_loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Optimizer 정의\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed74518-ff7c-4263-8e86-e4377d4cb7a5",
   "metadata": {},
   "source": [
    "## 6. 학습 스텝(`train_step`) 정의\n",
    "한 번의 배치(Batch)에 대한 학습 과정을 함수로 정의합니다. TensorFlow의 `@tf.function` 데코레이터를 사용하여 그래프 모드로 컴파일함으로써 학습 속도를 최적화합니다.\n",
    "\n",
    "이 함수 내에서 VAE와 Diffusion 모델의 손실을 각각 계산하고, Gradient를 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be9ce0-6372-466a-8faa-07d0e7d0f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # --- VAE 학습 ---\n",
    "        reconstructed, mean, log_var = vae(images)\n",
    "        vae_total_loss = vae_loss_fn(images, reconstructed, mean, log_var)\n",
    "\n",
    "        # --- Diffusion 모델 학습 ---\n",
    "        # 1. VAE 인코더로 실제 이미지를 잠재 벡터로 변환\n",
    "        true_latents, _ = vae.encoder(images)\n",
    "        \n",
    "        # 2. 노이즈 및 시간(t) 샘플링\n",
    "        noise = tf.random.normal(shape=tf.shape(true_latents))\n",
    "        # Epoch 기반으로 노이즈 레벨을 점진적으로 증가 (학습 안정화)\n",
    "        t = tf.random.uniform(shape=(BATCH_SIZE,), minval=0.0, maxval=current_epoch_norm, dtype=tf.float32)\n",
    "        t_reshaped = tf.reshape(t, (-1, 1))\n",
    "\n",
    "        # 3. 잠재 벡터에 노이즈 추가 (Forward Process)\n",
    "        noisy_latents = true_latents * (1.0 - t_reshaped) + noise * t_reshaped\n",
    "        \n",
    "        # 4. Denoiser가 노이즈 예측\n",
    "        predicted_noise = diffusion_model([noisy_latents, t])\n",
    "        diffusion_loss = mse_loss_fn(noise, predicted_noise)\n",
    "\n",
    "        # --- 최종 손실 ---\n",
    "        total_loss = vae_total_loss + diffusion_loss\n",
    "\n",
    "    # Gradient 계산 및 적용\n",
    "    trainable_vars = vae.trainable_variables + diffusion_model.trainable_variables\n",
    "    grads = tape.gradient(total_loss, trainable_vars)\n",
    "    optimizer.apply_gradients(zip(grads, trainable_vars))\n",
    "    \n",
    "    return total_loss, vae_total_loss, diffusion_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40180f-f5a6-4a77-a6ee-9b0d032680ba",
   "metadata": {},
   "source": [
    "## 7. 학습 루프 실행\n",
    "전체 데이터셋에 대해 `EPOCHS`만큼 반복하여 학습을 진행합니다. 매 Epoch마다 `train_step` 함수를 호출하여 모델을 업데이트하고, 주기적으로 생성된 이미지를 저장하여 학습 과정을 모니터링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d3a77-1317-4cd1-8f9d-32c04d27b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 생성 및 저장을 위한 헬퍼 함수\n",
    "def save_plot(images, epoch, save_dir):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(16):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow((images[i] + 1) / 2) # [-1, 1] -> [0, 1]로 변환하여 출력\n",
    "        plt.axis(\"off\")\n",
    "    plt.savefig(os.path.join(save_dir, f\"epoch_{epoch:03d}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "print(\"--- 학습 시작 ---\")\n",
    "for epoch in range(EPOCHS):\n",
    "    # 현재 epoch을 0~1 사이 값으로 정규화하여 노이즈 스케줄에 사용\n",
    "    current_epoch_norm = float(epoch) / float(EPOCHS) \n",
    "    \n",
    "    progress_bar = tqdm(train_dataset, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    total_loss_epoch, vae_loss_epoch, diff_loss_epoch = 0, 0, 0\n",
    "    \n",
    "    for step, images in enumerate(progress_bar):\n",
    "        total_loss, vae_loss, diff_loss = train_step(images)\n",
    "        total_loss_epoch += total_loss.numpy()\n",
    "        vae_loss_epoch += vae_loss.numpy()\n",
    "        diff_loss_epoch += diff_loss.numpy()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"Total Loss\": f\"{total_loss_epoch/(step+1):.4f}\",\n",
    "            \"VAE Loss\": f\"{vae_loss_epoch/(step+1):.4f}\",\n",
    "            \"Diffusion Loss\": f\"{diff_loss_epoch/(step+1):.4f}\"\n",
    "        })\n",
    "\n",
    "    # Epoch마다 생성된 이미지 샘플 저장\n",
    "    # Diffusion 과정 시뮬레이션: 랜덤 노이즈에서 시작하여 점진적으로 노이즈 제거\n",
    "    z_noise = tf.random.normal(shape=(16, LATENT_DIM))\n",
    "    current_latents = z_noise\n",
    "    \n",
    "    # 20 스텝에 걸쳐 노이즈 제거 (Reverse Process)\n",
    "    for t_step in np.linspace(current_epoch_norm, 0, 20):\n",
    "        t = tf.constant([t_step] * 16, dtype=tf.float32)\n",
    "        predicted_noise = diffusion_model([current_latents, t])\n",
    "        # 현재 잠재 벡터에서 예측된 노이즈를 빼서 잠재 벡터를 정제\n",
    "        current_latents -= (current_epoch_norm / 20) * predicted_noise\n",
    "        \n",
    "    generated_images = vae.decoder(current_latents)\n",
    "    save_plot(generated_images, epoch + 1, SAVE_DIR)\n",
    "\n",
    "    # 5 Epoch마다 모델 가중치 저장\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        vae.save_weights(os.path.join(SAVE_DIR, f\"vae_epoch_{epoch+1}.h5\"))\n",
    "        diffusion_model.save_weights(os.path.join(SAVE_DIR, f\"diffusion_epoch_{epoch+1}.h5\"))\n",
    "\n",
    "print(\"--- 학습 완료 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4edd88a-7244-4a88-8647-7704a9596f64",
   "metadata": {},
   "source": [
    "## 8. 최종 결과 생성\n",
    "학습이 완료된 모델을 사용하여 최종 이미지 그리드를 생성하고 파일로 저장합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f5c12-4361-4b80-8dff-0374b425edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 최종 이미지 생성 ---\")\n",
    "final_z_noise = tf.random.normal(shape=(64, LATENT_DIM))\n",
    "current_latents = final_z_noise\n",
    "final_epoch_norm = float(EPOCHS-1) / float(EPOCHS)\n",
    "\n",
    "# 100 스텝에 걸쳐 노이즈 제거하여 고품질 이미지 생성\n",
    "for t_step in tqdm(np.linspace(final_epoch_norm, 0, 100), desc=\"Generating Final Images\"):\n",
    "    t = tf.constant([t_step] * 64, dtype=tf.float32)\n",
    "    predicted_noise = diffusion_model([current_latents, t])\n",
    "    current_latents -= (final_epoch_norm / 100) * predicted_noise\n",
    "\n",
    "final_images = vae.decoder(current_latents)\n",
    "\n",
    "# 8x8 그리드로 이미지 저장\n",
    "plt.figure(figsize=(16, 16))\n",
    "for i in range(64):\n",
    "    plt.subplot(8, 8, i + 1)\n",
    "    plt.imshow((final_images[i] + 1) / 2)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"final_generated_grid.png\"))\n",
    "plt.show()\n",
    "\n",
    "print(f\"최종 이미지가 '{os.path.join(SAVE_DIR, 'final_generated_grid.png')}'에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
